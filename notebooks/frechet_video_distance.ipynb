{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "from typing import Any, Tuple, Optional, List, Callable\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(videos: tf.Tensor, target_resolution: Tuple[int, int]) -> Any:\n",
    "    \"\"\"\n",
    "    Run some preprocessing on the videos for I3D model.\n",
    "\n",
    "    :param videos: <T>[batch_size, num_frames, height, width, depth] The videos to be\n",
    "      preprocessed. We don't care about the specific dtype of the videos, it can\n",
    "      be anything that tf.image.resize_bilinear accepts. Values are expected to\n",
    "      be in the range 0-255.\n",
    "    :param target_resolution: (width, height): target video resolution\n",
    "    :return: videos: <float32>[batch_size, num_frames, height, width, depth]\n",
    "    \"\"\"\n",
    "\n",
    "    videos_shape = videos.shape.as_list()\n",
    "    all_frames = tf.reshape(videos, [-1] + videos_shape[-3:])\n",
    "    resized_videos = tf.image.resize_bilinear(all_frames, size=target_resolution)\n",
    "    target_shape = [videos_shape[0], -1] + list(target_resolution) + [3]\n",
    "    output_videos = tf.reshape(resized_videos, target_shape)\n",
    "    scaled_videos = 2.0 * tf.cast(output_videos, tf.float32) / 255.0 - 1\n",
    "    return scaled_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_in_graph(tensor_name: tf.Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether a given tensor does exists in the graph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "    except KeyError:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert video to numpy.\n",
    "    \"\"\"\n",
    "    generated = tensor.data.cpu().numpy()\n",
    "    generated[generated < -1] = -1\n",
    "    generated[generated > 1] = 1\n",
    "    generated = (generated + 1) / 2 * 255\n",
    "    return generated.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id3_embedding(videos: tf.Tensor, batch_size: int) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Embed the given videos using the Inflated 3D Convolution network.\n",
    "\n",
    "    Downloads the graph of the I3D from tf.hub and adds it to the graph on the\n",
    "    first call.\n",
    "\n",
    "    :param videos: <float32>[batch_size, num_frames, height=224, width=224, depth=3]. Expected range is [-1, 1].\n",
    "    :param batch_size: batch size\n",
    "    :return: <float32>[batch_size, embedding_size]. embedding_size depends on the model used.\n",
    "    :raises ValueError: when a provided embedding_layer is not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    module_spec = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\n",
    "\n",
    "    # Making sure that we import the graph separately for\n",
    "    # each different input video tensor.\n",
    "    module_name = \"fvd_kinetics-400_id3_module_\" + videos.name.replace(\":\", \"_\")\n",
    "\n",
    "    assert_ops = [\n",
    "        tf.Assert(tf.reduce_max(videos) <= 1.001, [\"max value in frame is > 1\", videos]),\n",
    "        tf.Assert(tf.reduce_min(videos) >= -1.001, [\"min value in frame is < -1\", videos]),\n",
    "        tf.assert_equal(tf.shape(videos)[0], batch_size, [\"invalid frame batch size: \", tf.shape(videos)], summarize=6),\n",
    "    ]\n",
    "    with tf.control_dependencies(assert_ops):\n",
    "        videos = tf.identity(videos)\n",
    "\n",
    "    module_scope = \"%s_apply_default/\" % module_name\n",
    "\n",
    "    # To check whether the module has already been loaded into the graph, we look\n",
    "    # for a given tensor name. If this tensor name exists, we assume the function\n",
    "    # has been called before and the graph was imported. Otherwise we import it.\n",
    "    # Note: in theory, the tensor could exist, but have wrong shapes.\n",
    "    # This will happen if create_id3_embedding is called with a frames_placehoder\n",
    "    # of wrong size/batch size, because even though that will throw a tf.Assert\n",
    "    # on graph-execution time, it will insert the tensor (with wrong shape) into\n",
    "    # the graph. This is why we need the following assert.\n",
    "    video_batch_size = int(videos.shape[0])\n",
    "    assert video_batch_size in [batch_size, -1, None], \"Invalid batch size\"\n",
    "    tensor_name = module_scope + \"RGB/inception_i3d/Mean:0\"\n",
    "    if not _is_in_graph(tensor_name):\n",
    "        i3d_model = hub.Module(module_spec, name=module_name)\n",
    "        i3d_model(videos)\n",
    "\n",
    "    # gets the kinetics-i3d-400-logits layer\n",
    "    tensor_name = module_scope + \"RGB/inception_i3d/Mean:0\"\n",
    "    tensor = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fvd(real_activations: tf.Tensor, generated_activations: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Return a list of ops that compute metrics as funcs of activations.\n",
    "\n",
    "    :param real_activations: <float32>[num_samples, embedding_size]\n",
    "    :param generated_activations: <float32>[num_samples, embedding_size]\n",
    "    :return: FVD score\n",
    "    \"\"\"\n",
    "    return tf.contrib.gan.eval.frechet_classifier_distance_from_activations(real_activations, generated_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackDataset(data.Dataset):\n",
    "    def __init__(self, root_path, img_size, nframes):\n",
    "        self.root_path = root_path\n",
    "        self.image_size = img_size\n",
    "        self.nframes = nframes\n",
    "        \n",
    "        self._transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.image_size, interpolation=Image.CUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.data = list(root_path.iterdir())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data[idx]\n",
    "        film_img = Image.open(path)\n",
    "        film = []\n",
    "        for i in range(self.nframes):\n",
    "            img = film_img.crop((img_size * i, 0, img_size * (i + 1), img_size))\n",
    "            film.append(img)\n",
    "\n",
    "        film = [self._transform(img) for img in film]\n",
    "        film = torch.stack(film).permute(1, 0, 2, 3)\n",
    "        return film\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(root_path: Path, output_path: Path, rows, columns, padding, img_size) -> Tuple:\n",
    "    for path in root_path.iterdir():\n",
    "        image = Image.open(path)\n",
    "        for r in range(rows):\n",
    "            images = []\n",
    "            for c in range(columns):\n",
    "                p = padding\n",
    "                size_col = p + (img_size + p) * c\n",
    "                size_row = p + (img_size + p) * r\n",
    "                img = image.crop((size_col, size_row, size_col + img_size, size_row + img_size))\n",
    "                images.append(np.asanyarray(img))\n",
    "            film = Image.fromarray(np.hstack(images))\n",
    "\n",
    "            film_output_path = output_path / f\"{path.stem}_{r}{path.suffix}\"\n",
    "            film_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            film.save(film_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakes_path = Path(\"results/clipping_frame24_iterD2_alphaSimil0_seed7879_220608-085346/fakes\")\n",
    "reals_path = Path(\"results/clipping_frame24_iterD2_alphaSimil0_seed7879_220608-085346/reals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 2\n",
    "rows = 3\n",
    "columns = 24\n",
    "img_size = 256\n",
    "frame = 5\n",
    "bs = 1\n",
    "bins = 200\n",
    "video_length = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(fakes_path, fakes_path.parent / \"fakes_split\", rows, columns, padding, img_size)\n",
    "split_data(reals_path, reals_path.parent / \"reals_split\", rows, columns, padding, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakesds = StackDataset(fakes_path, img_size, video_length)\n",
    "realsds = StackDataset(reals_path, img_size, video_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealBatchSampler:\n",
    "    \"\"\"\n",
    "    Wrapper for endless batch sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler: Any) -> None:\n",
    "        self._batch_size: int = sampler.batch_size\n",
    "        self._sampler = sampler\n",
    "        self._enumerator: Optional[Any] = None\n",
    "\n",
    "    def __iter__(self) -> Any:\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> Tuple:\n",
    "        \"\"\"\n",
    "        Sample provider call.\n",
    "        \"\"\"\n",
    "        if self._enumerator is None:\n",
    "            self._enumerator = enumerate(self._sampler)\n",
    "\n",
    "        batch_idx, batch = next(self._enumerator)\n",
    "\n",
    "        if batch_idx == len(self._sampler) - 1:\n",
    "            self._enumerator = enumerate(self._sampler)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get batch size.\n",
    "        \"\"\"\n",
    "        return self._batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get length.\n",
    "        \"\"\"\n",
    "        return len(self._sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, dataset, bs, num_workers):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def __call__(self):\n",
    "        sample_provider = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.bs,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        return RealBatchSampler(sample_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float16, shape=(1, video_length, img_size, img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "item1 = create_id3_embedding(preprocess(x, (224, 224)), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakes_loader = Loader(fakesds, bs, 4)()\n",
    "reals_loader = Loader(realsds, bs, 4)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0de81bd65e4722ae3f15b452062b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 400)\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/z1188643/miniconda3/envs/eval/lib/python3.6/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:701: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2106.1926\n"
     ]
    }
   ],
   "source": [
    "real_embeds, fake_embeds = [], []\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "for _ in tqdm(range(len(fakesds)), total=len(fakesds)):\n",
    "    fake_tuple, real_tuple = next(iter(fakes_loader)), next(iter(reals_loader))\n",
    "\n",
    "    fake_videos = to_numpy(fake_tuple[0])[None, :].transpose(0, 2, 3, 4, 1)\n",
    "    real_videos = to_numpy(real_tuple[0])[None, :].transpose(0, 2, 3, 4, 1)\n",
    "\n",
    "    lol1 = sess.run(item1, feed_dict={x: fake_videos})\n",
    "    lol2 = sess.run(item1, feed_dict={x: real_videos})\n",
    "\n",
    "    fake_embeds.append(lol1)\n",
    "    real_embeds.append(lol2)\n",
    "\n",
    "fake_videos = np.concatenate(fake_embeds)\n",
    "real_videos = np.concatenate(real_embeds)\n",
    "print(fake_videos.shape)\n",
    "fake_videos = tf.convert_to_tensor(fake_videos, np.float32)\n",
    "real_videos = tf.convert_to_tensor(real_videos, np.float32)\n",
    "result = calculate_fvd(fake_videos, real_videos).eval(session=sess)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('eval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44bee55eba464c428a92e6a33e8fa71068485112bceba86c87316a04ed9746c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
